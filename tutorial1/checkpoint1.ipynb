{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1 \n",
    "## (Do not remove any comments that start with\"# @@@\".) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: \n",
    "\n",
    "- You are being evaluated for completion and effort in this checkpoint. \n",
    "- Avoid manual labor / hard coding as much as possible, everything we've taught you so far are meant to simplify and automate your process.\n",
    "- Please do not remove any comment that starts with: \"# @@@\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the same `states_edu.csv` that you should already be familiar with from the tutorial.\n",
    "\n",
    "We investigated Grade 8 reading score in the tutorial. For this checkpoint, you are asked to investigate another test. Here's an overview:\n",
    "\n",
    "* Choose a specific response variable to focus on\n",
    ">Grade 4 Math, Grade 4 Reading, Grade 8 Math\n",
    "* Pick or create features to use\n",
    ">Will all the features be useful in predicting test score? Are some more important than others? Should you standardize, bin, or scale the data?\n",
    "* Explore the data as it relates to that test\n",
    ">Create at least 2 visualizations (graphs), each with a caption describing the graph and what it tells us about the data\n",
    "* Create training and testing data\n",
    ">Do you want to train on all the data? Only data from the last 10 years? Only Michigan data?\n",
    "* Train a ML model to predict outcome \n",
    ">Define what you want to predict, and pick a model in sklearn to use (see sklearn <a href=\"https://scikit-learn.org/stable/modules/linear_model.html\">regressors</a>).\n",
    "\n",
    "\n",
    "Include comments throughout your code! Every cleanup and preprocessing task should be documented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Cleanup </h2>\n",
    "\n",
    "Import `numpy`, `pandas`, and `matplotlib`.\n",
    "\n",
    "(Feel free to import other libraries!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the \"states_edu.csv\" dataset and take a look at the head of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/states_edu.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always familiarize yourself with what each column in the dataframe represents. Read about the states_edu dataset here: https://www.kaggle.com/noriuk/us-education-datasets-unification-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this space to rename columns, deal with missing data, etc. _(optional)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploratory Data Analysis (EDA) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen one of Grade 4 Reading, Grade 4 Math, or Grade 8 Math to focus on: *ENTER YOUR CHOICE HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many years of data are logged in our dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of years: 33\n"
     ]
    }
   ],
   "source": [
    "# @@@ 1\n",
    "num_years = df[\"YEAR\"].nunique()\n",
    "print(\"Number of years:\", num_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Michigan to Ohio. Which state has the higher average across all years in the test you chose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 2\n",
    "mi_avg = df[df[\"STATE\"] == \"MICHIGAN\"][\"AVG_READING_4_SCORE\"].mean()\n",
    "oh_avg = df[df[\"STATE\"] == \"OHIO\"][\"AVG_READING_4_SCORE\"].mean()\n",
    "print(\"Michigan Avg:\", mi_avg, \"Ohio Avg:\", oh_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average for your chosen test across all states in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationwide avg in 2019: 219.79245283018867\n"
     ]
    }
   ],
   "source": [
    "# @@@ 3\n",
    "avg_2019 = df[df[\"YEAR\"] == 2019][\"AVG_READING_4_SCORE\"].mean()\n",
    "print(\"Nationwide avg in 2019:\", avg_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state, find a maximum value for your chosen test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE\n",
      "ALABAMA                 220.0\n",
      "ALASKA                  214.0\n",
      "ARIZONA                 216.0\n",
      "ARKANSAS                219.0\n",
      "CALIFORNIA              216.0\n",
      "COLORADO                227.0\n",
      "CONNECTICUT             230.0\n",
      "DELAWARE                226.0\n",
      "DISTRICT_OF_COLUMBIA    214.0\n",
      "DODEA                   235.0\n",
      "FLORIDA                 228.0\n",
      "GEORGIA                 222.0\n",
      "HAWAII                  218.0\n",
      "IDAHO                   223.0\n",
      "ILLINOIS                222.0\n",
      "INDIANA                 227.0\n",
      "IOWA                    225.0\n",
      "KANSAS                  225.0\n",
      "KENTUCKY                228.0\n",
      "LOUISIANA               216.0\n",
      "MAINE                   228.0\n",
      "MARYLAND                232.0\n",
      "MASSACHUSETTS           237.0\n",
      "MICHIGAN                220.0\n",
      "MINNESOTA               227.0\n",
      "MISSISSIPPI             219.0\n",
      "MISSOURI                224.0\n",
      "MONTANA                 227.0\n",
      "NATIONAL                223.0\n",
      "NEBRASKA                227.0\n",
      "NEVADA                  218.0\n",
      "NEW_HAMPSHIRE           232.0\n",
      "NEW_JERSEY              233.0\n",
      "NEW_MEXICO              212.0\n",
      "NEW_YORK                224.0\n",
      "NORTH_CAROLINA          226.0\n",
      "NORTH_DAKOTA            226.0\n",
      "OHIO                    226.0\n",
      "OKLAHOMA                222.0\n",
      "OREGON                  220.0\n",
      "PENNSYLVANIA            227.0\n",
      "RHODE_ISLAND            225.0\n",
      "SOUTH_CAROLINA          218.0\n",
      "SOUTH_DAKOTA            223.0\n",
      "TENNESSEE               220.0\n",
      "TEXAS                   220.0\n",
      "UTAH                    226.0\n",
      "VERMONT                 230.0\n",
      "VIRGINIA                229.0\n",
      "WASHINGTON              226.0\n",
      "WEST_VIRGINIA           219.0\n",
      "WISCONSIN               224.0\n",
      "WYOMING                 228.0\n",
      "Name: AVG_READING_4_SCORE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# @@@ 4\n",
    "max_by_state = df.groupby(\"STATE\")[\"AVG_READING_4_SCORE\"].max()\n",
    "print(max_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refer to the `Grouping and Aggregating` section in Tutorial 0 if you are stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Engineering </h2>\n",
    "\n",
    "After exploring the data, you can choose to modify features that you would use to predict the performance of the students on your chosen response variable. \n",
    "\n",
    "You can also create your own features. For example, perhaps you figured that maybe a state's expenditure per student may affect their overall academic performance so you create a expenditure_per_student feature.\n",
    "\n",
    "Use this space to modify or create features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering justification: **<BRIEFLY DESCRIBE WHY YOU MADE THE CHANGES THAT YOU DID\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization</h2>\n",
    "\n",
    "Investigate the relationship between your chosen response variable and at least two predictors using visualizations. Write down your observations.\n",
    "\n",
    "**Visualization 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 6\n",
    "df[\"AVG_READING_4_SCORE\"].plot.hist(edgecolor=\"black\", title=\"Distribution of 4th Grade Reading Scores\")\n",
    "plt.xlabel(\"Grade 4 Reading Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<CAPTION FOR VIZ 1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 7\n",
    "df.plot.scatter(x=\"EXPENDITURE_PER_STUDENT\", y=\"AVG_READING_4_SCORE\", alpha=0.6)\n",
    "plt.xlabel(\"Expenditure per Student\")\n",
    "plt.ylabel(\"Grade 4 Reading Score\")\n",
    "plt.title(\"Expenditure vs. 4th Grade Reading\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<CAPTION FOR VIZ 2>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Creation </h2>\n",
    "\n",
    "_Use this space to create train/test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 8\n",
    "\n",
    "X = df[[\"EXPENDITURE_PER_STUDENT\", \"INSTRUCTION_EXPENDITURE\", \"SUPPORT_SERVICES_EXPENDITURE\"]].dropna()\n",
    "y = df.loc[X.index][\"AVG_READING_4_SCORE\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 9 \n",
    "\n",
    "y.fillna(y.median(), inplace=True)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Models [Resource](https://medium.com/@vijaya.beeravalli/comparison-of-machine-learning-classification-models-for-credit-card-default-data-c3cf805c9a5a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 10\n",
    "# import your sklearn class here\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 11\n",
    "# create your model here\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose some metrics to evaluate the performance of your model, some of them are mentioned in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 12\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"R^2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error:\", mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have copied over the graphs that visualize the model's performance on the training and testing set. \n",
    "\n",
    "Change `col_name` and modify the call to `plt.ylabel()` to isolate how a single predictor affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ 13\n",
    "\n",
    "col_name = \"EXPENDITURE_PER_STUDENT\"\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_train[col_name], y_train, color=\"red\", alpha=0.6)\n",
    "plt.scatter(X_train[col_name], model.predict(X_train), color=\"green\", alpha=0.6)\n",
    "plt.legend([\"True Training\", \"Predicted Training\"])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel(\"Grade 4 Reading Score\")\n",
    "plt.title(\"Model Behavior On Training Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m col_name = \u001b[33m\"\u001b[39m\u001b[33mEXPENDITURE_PER_STUDENT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m f = plt.figure(figsize=(\u001b[32m12\u001b[39m,\u001b[32m6\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m plt.scatter(\u001b[43mX_test\u001b[49m[col_name], y_test, color=\u001b[33m\"\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m\"\u001b[39m, alpha=\u001b[32m0.6\u001b[39m)\n\u001b[32m      8\u001b[39m plt.scatter(X_test[col_name], model.predict(X_test), color=\u001b[33m\"\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m\"\u001b[39m, alpha=\u001b[32m0.6\u001b[39m)\n\u001b[32m      9\u001b[39m plt.legend([\u001b[33m\"\u001b[39m\u001b[33mTrue Testing\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPredicted Testing\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @@@ 14\n",
    "\n",
    "\n",
    "col_name = \"EXPENDITURE_PER_STUDENT\"\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_test[col_name], y_test, color=\"blue\", alpha=0.6)\n",
    "plt.scatter(X_test[col_name], model.predict(X_test), color=\"black\", alpha=0.6)\n",
    "plt.legend([\"True Testing\", \"Predicted Testing\"])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel(\"Grade 4 Reading Score\")\n",
    "plt.title(\"Model Behavior On Testing Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6cf8df3ff69f85f626faf55c10df6fe2cb9d1236b4dc73844ee4dc01369c2c99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
